{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# Assignment Module 2: Product Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTQUJ4uYH1w"
      },
      "source": [
        "## Preliminaries: the dataset\n",
        "\n",
        "The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "The products belong to the following 43 classes:\n",
        "```\n",
        "0.  Apple\n",
        "1.  Avocado\n",
        "2.  Banana\n",
        "3.  Kiwi\n",
        "4.  Lemon\n",
        "5.  Lime\n",
        "6.  Mango\n",
        "7.  Melon\n",
        "8.  Nectarine\n",
        "9.  Orange\n",
        "10. Papaya\n",
        "11. Passion-Fruit\n",
        "12. Peach\n",
        "13. Pear\n",
        "14. Pineapple\n",
        "15. Plum\n",
        "16. Pomegranate\n",
        "17. Red-Grapefruit\n",
        "18. Satsumas\n",
        "19. Juice\n",
        "20. Milk\n",
        "21. Oatghurt\n",
        "22. Oat-Milk\n",
        "23. Sour-Cream\n",
        "24. Sour-Milk\n",
        "25. Soyghurt\n",
        "26. Soy-Milk\n",
        "27. Yoghurt\n",
        "28. Asparagus\n",
        "29. Aubergine\n",
        "30. Cabbage\n",
        "31. Carrots\n",
        "32. Cucumber\n",
        "33. Garlic\n",
        "34. Ginger\n",
        "35. Leek\n",
        "36. Mushroom\n",
        "37. Onion\n",
        "38. Pepper\n",
        "39. Potato\n",
        "40. Red-Beet\n",
        "41. Tomato\n",
        "42. Zucchini\n",
        "```\n",
        "\n",
        "The dataset is split into training (`train`), validation (`val`), and test (`test`) set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pdrmJRnJPd8"
      },
      "source": [
        "The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:14.579828Z",
          "iopub.status.busy": "2024-12-11T14:38:14.579352Z",
          "iopub.status.idle": "2024-12-11T14:38:47.860259Z",
          "shell.execute_reply": "2024-12-11T14:38:47.858901Z",
          "shell.execute_reply.started": "2024-12-11T14:38:14.579787Z"
        },
        "id": "POMX_3x-_bZI",
        "outputId": "cc0f1741-29d6-4144-9453-0e785924ef4b",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroceryStoreDataset'...\n",
            "remote: Enumerating objects: 6559, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 6559 (delta 45), reused 37 (delta 35), pack-reused 6293 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6559/6559), 116.26 MiB | 33.42 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n",
            "Updating files: 100% (5717/5717), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.863281Z",
          "iopub.status.busy": "2024-12-11T14:38:47.862841Z",
          "iopub.status.idle": "2024-12-11T14:38:47.870934Z",
          "shell.execute_reply": "2024-12-11T14:38:47.869714Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.863236Z"
        },
        "id": "hiF8xGEYlsu8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from typing import List, Tuple\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.872796Z",
          "iopub.status.busy": "2024-12-11T14:38:47.872297Z",
          "iopub.status.idle": "2024-12-11T14:38:47.890664Z",
          "shell.execute_reply": "2024-12-11T14:38:47.888544Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.872741Z"
        },
        "id": "jROSO2qVDxdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GroceryStoreDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = Path(\"/content/GroceryStoreDataset/dataset\")\n",
        "        self.split = split\n",
        "        self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                # path, fine-grained class, coarse-grained class\n",
        "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
        "                paths.append(path), labels.append(int(label))\n",
        "\n",
        "        return paths, labels\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBch3dpwNSsW"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n",
        "\n",
        "- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n7OMjXBaDiy"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.894407Z",
          "iopub.status.busy": "2024-12-11T14:38:47.893948Z",
          "iopub.status.idle": "2024-12-11T14:38:47.910558Z",
          "shell.execute_reply": "2024-12-11T14:38:47.909209Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.894366Z"
        },
        "id": "W0XqMhoDaDiz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTJ2g6w3aDiz"
      },
      "source": [
        "### data preprocessing and augmentation\n",
        "In this first step we transform both the train and the validation sets, this is done because we want to do some augmentation in order to reduce the bias of the network.\n",
        "- at first the images are resized to 64x64 in order to ensure uniform input size\n",
        "- then we have the data augmentation (of course, only for the training set) with RandomHorizontalFlip and RandomRotation\n",
        "- finally we normalize the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.912615Z",
          "iopub.status.busy": "2024-12-11T14:38:47.912140Z",
          "iopub.status.idle": "2024-12-11T14:38:47.925379Z",
          "shell.execute_reply": "2024-12-11T14:38:47.924076Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.912568Z"
        },
        "id": "kMGmy1mlaDiz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Data augmentation and normalization\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_5APoSYSTePY"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.927843Z",
          "iopub.status.busy": "2024-12-11T14:38:47.927426Z",
          "iopub.status.idle": "2024-12-11T14:38:47.959544Z",
          "shell.execute_reply": "2024-12-11T14:38:47.958324Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.927805Z"
        },
        "id": "JI5KKJTCaDi0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 1: Define the device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\n",
        "val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n",
        "test_dataset = GroceryStoreDataset(split='test', transform=test_transforms)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAFgYzQUaDi0"
      },
      "source": [
        "### plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.961460Z",
          "iopub.status.busy": "2024-12-11T14:38:47.961064Z",
          "iopub.status.idle": "2024-12-11T14:38:47.972366Z",
          "shell.execute_reply": "2024-12-11T14:38:47.970931Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.961425Z"
        },
        "id": "clgRrpT3aDi0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_acc, val_acc):\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(train_loss, val_loss):\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGCeS6BraDi0"
      },
      "source": [
        "### definition of training function\n",
        "Here we define the training function. It takes as input the model, the train and val loader, as well as the criterion, optimizer, scheduler, number of epochs and patience. The total number of epoch is set to 20, although if there is no improvement after 5 steps the model stops training. At each epoch, we train on the train set and then evaluate the performance on the validation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.974040Z",
          "iopub.status.busy": "2024-12-11T14:38:47.973610Z",
          "iopub.status.idle": "2024-12-11T14:38:47.990674Z",
          "shell.execute_reply": "2024-12-11T14:38:47.989309Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.973964Z"
        },
        "id": "AAbfRr9gaDi0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model_with_early_stopping_and_scheduler(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
        "    \"\"\"\n",
        "    Train the model on the training set and evaluate it on the validation set, with early stopping and a learning rate scheduler.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        criterion (nn.Module): Loss function used to calculate training and validation loss.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model weight updates.\n",
        "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler to adjust the learning rate during training.\n",
        "        num_epochs (int, optional): Maximum number of training epochs. Default is 20.\n",
        "        patience (int, optional): Number of consecutive epochs without validation accuracy improvement before early stopping. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The trained model with the best weights (based on validation accuracy).\n",
        "        train_loss_arr (list): List of training loss values for each epoch.\n",
        "        train_acc_arr (list): List of training accuracy values for each epoch.\n",
        "        val_loss_arr (list): List of validation loss values for each epoch.\n",
        "        val_acc_arr (list): List of validation accuracy values for each epoch.\n",
        "    \"\"\"\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    early_stop = False\n",
        "\n",
        "    train_loss_arr = []\n",
        "    train_acc_arr = []\n",
        "    val_loss_arr = []\n",
        "    val_acc_arr = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        train_loss_arr.append(epoch_loss)\n",
        "        train_acc_arr.append(epoch_acc)\n",
        "\n",
        "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "        val_loss_arr.append(val_loss)\n",
        "        val_acc_arr.append(val_acc)\n",
        "\n",
        "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Check if early stopping condition is met\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # Restore best model weights\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ct1hbRVaDi1"
      },
      "source": [
        "### definition of evaluation function\n",
        "This function takes as input the model, the test loader and the criterion. It calculates the average test loss and the test accuracy by comparing the computed labels with the true ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:47.992608Z",
          "iopub.status.busy": "2024-12-11T14:38:47.992248Z",
          "iopub.status.idle": "2024-12-11T14:38:48.010258Z",
          "shell.execute_reply": "2024-12-11T14:38:48.009042Z",
          "shell.execute_reply.started": "2024-12-11T14:38:47.992574Z"
        },
        "id": "T87fGfFyaDi1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def evaluate_model_on_test_set(model, test_loader, criterion):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        criterion (nn.Module): Loss function to calculate test loss.\n",
        "\n",
        "    Returns:\n",
        "        test_loss (float): Average loss on the test set.\n",
        "        test_accuracy (float): Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    test_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    test_loss = test_loss / total_samples\n",
        "    test_accuracy = test_corrects.double() / total_samples\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}')\n",
        "    return test_loss, test_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obRcxvjraDi1"
      },
      "source": [
        "### definition of loss function\n",
        "In this class we define a variation of CrossEntropyLoss which also incorporates label smoothing, which is a regularization technique that adjusts the target labels during training by distributing a small portion of the target's \"probability mass\" to other classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:48.017497Z",
          "iopub.status.busy": "2024-12-11T14:38:48.017100Z",
          "iopub.status.idle": "2024-12-11T14:38:48.032693Z",
          "shell.execute_reply": "2024-12-11T14:38:48.031358Z",
          "shell.execute_reply.started": "2024-12-11T14:38:48.017460Z"
        },
        "id": "XVFHHvsLaDi2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Label Smoothing Cross Entropy Loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Get number of classes\n",
        "        num_classes = pred.size(1)\n",
        "\n",
        "        # Create smoothed labels\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * F.log_softmax(pred, dim=1), dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMo58tRnaDi2"
      },
      "source": [
        "### definition of the network\n",
        "The following is a CNN which is called EnhancedCNN because its predecessor, the SimpleCNN, of course was extremely basic and didn't perform as well.\n",
        "This network uses 2d convolutions, with 64, 128, 256, 512 filters. We choose the convolutional layers because it was requested by the task assignment (aka, build a convolutional neural network). The activation function is ReLU.\n",
        "\n",
        "We also use batch normalization and max pooling after the convolutional layers. Batch normalization is used specifically because it speeds up convergence and improves generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:48.034862Z",
          "iopub.status.busy": "2024-12-11T14:38:48.034377Z",
          "iopub.status.idle": "2024-12-11T14:38:48.051654Z",
          "shell.execute_reply": "2024-12-11T14:38:48.049848Z",
          "shell.execute_reply.started": "2024-12-11T14:38:48.034812Z"
        },
        "id": "F2C6MADeaDi2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EnhancedCNN, self).__init__()\n",
        "        # First convolutional block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Global Average Pooling layer instead of flattening\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Output size of 1x1\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 1024)  # Adjust based on the output from the global avg pooling\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First block\n",
        "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
        "\n",
        "        # Second block\n",
        "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.batchnorm4(self.conv4(x))))\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.global_avg_pool(x)  # Output size becomes [batch_size, 512, 1, 1]\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor to [batch_size, 512]\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dD8TKGeaDi2"
      },
      "source": [
        "### Training the model\n",
        "As criterion we are using the **LabelSmoothingLoss** priorly defined, with smoothing parameter = 0.1. As an optimizer we are using **Adam**, which optimizes the model's weights in order to minimize the loss function. The parameter lr indicates the learning rate and sets the initial learning rate. Then the learning rate is adjusted via a scheduler, **StepLR** which dynamically adjusts the learning rate during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T14:38:48.053738Z",
          "iopub.status.busy": "2024-12-11T14:38:48.053377Z",
          "iopub.status.idle": "2024-12-11T15:03:02.292883Z",
          "shell.execute_reply": "2024-12-11T15:03:02.289898Z",
          "shell.execute_reply.started": "2024-12-11T14:38:48.053704Z"
        },
        "id": "btAzLgUAaDi2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "num_classes = train_dataset.get_num_classes()\n",
        "model = EnhancedCNN(num_classes).to(device)\n",
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\n",
        "# Train the model with early stopping\n",
        "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, patience=5)\n",
        "\n",
        "# Save the best model\n",
        "torch.save(trained_model.state_dict(), 'best_grocery_cnn.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q74-g1hlaDi3"
      },
      "source": [
        "### plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-11T15:03:02.957652Z",
          "iopub.status.busy": "2024-12-11T15:03:02.957309Z",
          "iopub.status.idle": "2024-12-11T15:03:32.497857Z",
          "shell.execute_reply": "2024-12-11T15:03:32.496715Z",
          "shell.execute_reply.started": "2024-12-11T15:03:02.957617Z"
        },
        "id": "j5WiUP0uaDi3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_grocery_cnn.pth'))\n",
        "model.to(device)\n",
        "test_loss, test_accuracy = evaluate_model_on_test_set(model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkWEqSPoUIL3"
      },
      "source": [
        "## Part 2: fine-tune an existing network\n",
        "\n",
        "Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
        "\n",
        "1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n",
        "1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TFK0rQKaDi4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import copy\n",
        "from torchvision import transforms\n",
        "\n",
        "# Step 1: Define device\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 2: Load pretrained ResNet-18 model\n",
        "#resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  # Load pretrained ResNet-18\n",
        "\n",
        "# Step 3: Modify the classifier to fit the GroceryStoreDataset\n",
        "#num_classes = train_dataset.get_num_classes()  # Dynamically fetch the correct number of classes\n",
        "#resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "#resnet18 = resnet18.to(device)\n",
        "\n",
        "# Step 4: Define training and validation transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 5: Load datasets and dataloaders\n",
        "train_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\n",
        "val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Step 2: Load pretrained ResNet-18 model\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Step 3: Modify the classifier to fit the GroceryStoreDataset\n",
        "num_classes = train_dataset.get_num_classes()  # Dynamically fetch the correct number of classes\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "# Step 6: Define the loss function, optimizer, and learning rate scheduler\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
        "#scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\n",
        "# Train the ResNet-18 model\n",
        "resnet18, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
        "    model=resnet18,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=30,\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(resnet18.state_dict(), 'fine_tuned_resnet18.pth')\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TRbcsRb_zvZn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch import Tensor\n",
        "from torchsummary import summary\n",
        "from torch.optim import Adam, lr_scheduler, SGD\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def get_data_loaders(train_transform, val_test_transform, batch_size=32):\n",
        "\n",
        "    train_dataset = GroceryStoreDataset(split='train', transform=train_transform)\n",
        "    val_dataset = GroceryStoreDataset(split='val', transform=val_test_transform)\n",
        "    test_dataset = GroceryStoreDataset(split='test', transform=val_test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_loader, val_loader, _ = get_data_loaders(train_transform=train_transforms, val_test_transform=val_transforms)\n",
        "\n",
        "all_models = {}\n",
        "\n",
        "def get_model():\n",
        "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    num_classes = train_dataset.get_num_classes()\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DzLMDjIw0v_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a765c4-22d3-4bcd-d5fe-cbc9d99f8aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 146MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = get_model().to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'fc' in name:  # If it's part of the fully connected layer\n",
        "        param.requires_grad = True  # Keep it trainable\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze all other layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VcVhD6dz0xdH"
      },
      "outputs": [],
      "source": [
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "num_epochs = 30\n",
        "patience = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L3lokTY1FqC",
        "outputId": "123a3bec-288b-451e-8f8d-97734cbd36ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Training Loss: 2.7267 Acc: 0.3625\n",
            "Validation Loss: 2.3518 Acc: 0.4257\n",
            "--------------------\n",
            "Epoch 2/30\n",
            "Training Loss: 1.7511 Acc: 0.6947\n",
            "Validation Loss: 1.9336 Acc: 0.5541\n",
            "--------------------\n",
            "Epoch 3/30\n",
            "Training Loss: 1.4229 Acc: 0.8170\n",
            "Validation Loss: 1.7694 Acc: 0.6351\n",
            "--------------------\n",
            "Epoch 4/30\n",
            "Training Loss: 1.2808 Acc: 0.8693\n",
            "Validation Loss: 1.7370 Acc: 0.6419\n",
            "--------------------\n",
            "Epoch 5/30\n",
            "Training Loss: 1.1839 Acc: 0.9076\n",
            "Validation Loss: 1.6506 Acc: 0.6858\n",
            "--------------------\n",
            "Epoch 6/30\n",
            "Training Loss: 1.1437 Acc: 0.9170\n",
            "Validation Loss: 1.5809 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 7/30\n",
            "Training Loss: 1.1051 Acc: 0.9261\n",
            "Validation Loss: 1.5748 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 8/30\n",
            "Training Loss: 1.0656 Acc: 0.9420\n",
            "Validation Loss: 1.6145 Acc: 0.6993\n",
            "--------------------\n",
            "Epoch 9/30\n",
            "Training Loss: 1.0446 Acc: 0.9462\n",
            "Validation Loss: 1.6160 Acc: 0.6926\n",
            "--------------------\n",
            "Epoch 10/30\n",
            "Training Loss: 1.0217 Acc: 0.9508\n",
            "Validation Loss: 1.5848 Acc: 0.7196\n",
            "--------------------\n",
            "Epoch 11/30\n",
            "Training Loss: 1.0057 Acc: 0.9557\n",
            "Validation Loss: 1.5516 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 12/30\n",
            "Training Loss: 0.9960 Acc: 0.9678\n",
            "Validation Loss: 1.5379 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 13/30\n",
            "Training Loss: 0.9835 Acc: 0.9705\n",
            "Validation Loss: 1.5494 Acc: 0.7196\n",
            "--------------------\n",
            "Epoch 14/30\n",
            "Training Loss: 0.9907 Acc: 0.9640\n",
            "Validation Loss: 1.5378 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 15/30\n",
            "Training Loss: 0.9897 Acc: 0.9625\n",
            "Validation Loss: 1.5370 Acc: 0.7399\n",
            "--------------------\n",
            "Epoch 16/30\n",
            "Training Loss: 0.9819 Acc: 0.9701\n",
            "Validation Loss: 1.5276 Acc: 0.7432\n",
            "--------------------\n",
            "Epoch 17/30\n",
            "Training Loss: 0.9902 Acc: 0.9625\n",
            "Validation Loss: 1.5503 Acc: 0.7466\n",
            "--------------------\n",
            "Epoch 18/30\n",
            "Training Loss: 0.9763 Acc: 0.9727\n",
            "Validation Loss: 1.5452 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 19/30\n",
            "Training Loss: 0.9798 Acc: 0.9689\n",
            "Validation Loss: 1.5420 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 20/30\n",
            "Training Loss: 0.9793 Acc: 0.9705\n",
            "Validation Loss: 1.5494 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 21/30\n",
            "Training Loss: 0.9722 Acc: 0.9674\n",
            "Validation Loss: 1.5229 Acc: 0.7500\n",
            "--------------------\n",
            "Epoch 22/30\n",
            "Training Loss: 0.9765 Acc: 0.9674\n",
            "Validation Loss: 1.5320 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 23/30\n",
            "Training Loss: 0.9694 Acc: 0.9712\n",
            "Validation Loss: 1.5328 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 24/30\n",
            "Training Loss: 0.9749 Acc: 0.9659\n",
            "Validation Loss: 1.5443 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 25/30\n",
            "Training Loss: 0.9761 Acc: 0.9667\n",
            "Validation Loss: 1.5432 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 26/30\n",
            "Training Loss: 0.9700 Acc: 0.9742\n",
            "Validation Loss: 1.5335 Acc: 0.7264\n",
            "Early stopping triggered\n",
            "Best val Acc: 0.7500\n"
          ]
        }
      ],
      "source": [
        "#model_save_path='resnet18-v1.pth'\n",
        "\n",
        "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    patience=patience\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(trained_model.state_dict(), 'resnet18-v1.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2 - Tweak the hyperparameters"
      ],
      "metadata": {
        "id": "i7IB_EiCxP5x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctWIVCEYaDi5"
      },
      "source": [
        "#### Attempt 1\n",
        "- change the learning rate; higher for the final layer and lower for pretrained layers\n",
        "- increase patience"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model().to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'fc' in name:  # If it's part of the fully connected layer\n",
        "        param.requires_grad = True  # Keep it trainable\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze all other layers"
      ],
      "metadata": {
        "id": "h17VYlUbxoly"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "#optimizer = Adam(model.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3},  # Higher LR for final layer\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4}])  # Lower LR for pretrained layers\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "num_epochs = 30\n",
        "patience = 10"
      ],
      "metadata": {
        "id": "7adk-QitxdkI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_save_path='resnet18-v2.pth'\n",
        "\n",
        "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    patience=patience\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(trained_model.state_dict(), 'resnet18-v2.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB_15Jgfxpb0",
        "outputId": "83ec4bc1-6122-4b78-e272-dc544bf796a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Training Loss: 2.6671 Acc: 0.3883\n",
            "Validation Loss: 2.2631 Acc: 0.4527\n",
            "--------------------\n",
            "Epoch 2/30\n",
            "Training Loss: 1.7323 Acc: 0.6826\n",
            "Validation Loss: 1.9393 Acc: 0.5676\n",
            "--------------------\n",
            "Epoch 3/30\n",
            "Training Loss: 1.4136 Acc: 0.8288\n",
            "Validation Loss: 1.7186 Acc: 0.6858\n",
            "--------------------\n",
            "Epoch 4/30\n",
            "Training Loss: 1.2682 Acc: 0.8731\n",
            "Validation Loss: 1.6852 Acc: 0.6622\n",
            "--------------------\n",
            "Epoch 5/30\n",
            "Training Loss: 1.1928 Acc: 0.8981\n",
            "Validation Loss: 1.6246 Acc: 0.7027\n",
            "--------------------\n",
            "Epoch 6/30\n",
            "Training Loss: 1.1374 Acc: 0.9261\n",
            "Validation Loss: 1.6221 Acc: 0.7128\n",
            "--------------------\n",
            "Epoch 7/30\n",
            "Training Loss: 1.0976 Acc: 0.9326\n",
            "Validation Loss: 1.5897 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 8/30\n",
            "Training Loss: 1.0612 Acc: 0.9428\n",
            "Validation Loss: 1.5573 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 9/30\n",
            "Training Loss: 1.0431 Acc: 0.9477\n",
            "Validation Loss: 1.6378 Acc: 0.6926\n",
            "--------------------\n",
            "Epoch 10/30\n",
            "Training Loss: 1.0245 Acc: 0.9515\n",
            "Validation Loss: 1.5817 Acc: 0.7061\n",
            "--------------------\n",
            "Epoch 11/30\n",
            "Training Loss: 0.9979 Acc: 0.9629\n",
            "Validation Loss: 1.5618 Acc: 0.7196\n",
            "--------------------\n",
            "Epoch 12/30\n",
            "Training Loss: 0.9912 Acc: 0.9674\n",
            "Validation Loss: 1.5446 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 13/30\n",
            "Training Loss: 0.9912 Acc: 0.9595\n",
            "Validation Loss: 1.5505 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 14/30\n",
            "Training Loss: 0.9891 Acc: 0.9663\n",
            "Validation Loss: 1.5522 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 15/30\n",
            "Training Loss: 0.9793 Acc: 0.9701\n",
            "Validation Loss: 1.5577 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 16/30\n",
            "Training Loss: 0.9920 Acc: 0.9629\n",
            "Validation Loss: 1.5644 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 17/30\n",
            "Training Loss: 0.9835 Acc: 0.9614\n",
            "Validation Loss: 1.5397 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 18/30\n",
            "Training Loss: 0.9811 Acc: 0.9648\n",
            "Validation Loss: 1.5572 Acc: 0.7196\n",
            "--------------------\n",
            "Epoch 19/30\n",
            "Training Loss: 0.9850 Acc: 0.9655\n",
            "Validation Loss: 1.5459 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 20/30\n",
            "Training Loss: 0.9808 Acc: 0.9659\n",
            "Validation Loss: 1.5349 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 21/30\n",
            "Training Loss: 0.9814 Acc: 0.9663\n",
            "Validation Loss: 1.5355 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 22/30\n",
            "Training Loss: 0.9754 Acc: 0.9705\n",
            "Validation Loss: 1.5368 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 23/30\n",
            "Training Loss: 0.9792 Acc: 0.9636\n",
            "Validation Loss: 1.5389 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 24/30\n",
            "Training Loss: 0.9843 Acc: 0.9648\n",
            "Validation Loss: 1.5342 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 25/30\n",
            "Training Loss: 0.9656 Acc: 0.9712\n",
            "Validation Loss: 1.5469 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 26/30\n",
            "Training Loss: 0.9837 Acc: 0.9640\n",
            "Validation Loss: 1.5376 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 27/30\n",
            "Training Loss: 0.9787 Acc: 0.9689\n",
            "Validation Loss: 1.5190 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 28/30\n",
            "Training Loss: 0.9828 Acc: 0.9591\n",
            "Validation Loss: 1.5284 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 29/30\n",
            "Training Loss: 0.9747 Acc: 0.9659\n",
            "Validation Loss: 1.5369 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 30/30\n",
            "Training Loss: 0.9648 Acc: 0.9701\n",
            "Validation Loss: 1.5452 Acc: 0.7331\n",
            "--------------------\n",
            "Best val Acc: 0.7365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 2: change the learning rate for the intermediate layers"
      ],
      "metadata": {
        "id": "6ZoftlBPS4bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3},       # Higher learning rate for final layer\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4},   # Intermediate for higher layers\n",
        "    {'params': model.layer3.parameters(), 'lr': 1e-5}    # Lower for earlier layers\n",
        "])\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "num_epochs = 30\n",
        "patience = 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y38CiOapxp5Q",
        "outputId": "47ff1be5-dd84-4ddb-f63b-685bdfb31e36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 83.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_save_path='resnet18-v3.pth'\n",
        "\n",
        "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    patience=patience\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(trained_model.state_dict(), 'resnet18-v3.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznKU3ACTLAB",
        "outputId": "7b1105c6-ea26-4204-fbb3-86b6502a4f15"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Training Loss: 1.0100 Acc: 0.9591\n",
            "Validation Loss: 1.6077 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 2/30\n",
            "Training Loss: 0.9935 Acc: 0.9610\n",
            "Validation Loss: 1.5597 Acc: 0.7162\n",
            "--------------------\n",
            "Epoch 3/30\n",
            "Training Loss: 0.9815 Acc: 0.9595\n",
            "Validation Loss: 1.5811 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 4/30\n",
            "Training Loss: 0.9777 Acc: 0.9640\n",
            "Validation Loss: 1.5641 Acc: 0.7095\n",
            "--------------------\n",
            "Epoch 5/30\n",
            "Training Loss: 0.9628 Acc: 0.9667\n",
            "Validation Loss: 1.4937 Acc: 0.7635\n",
            "--------------------\n",
            "Epoch 6/30\n",
            "Training Loss: 0.9576 Acc: 0.9682\n",
            "Validation Loss: 1.5073 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 7/30\n",
            "Training Loss: 0.9647 Acc: 0.9667\n",
            "Validation Loss: 1.5412 Acc: 0.7365\n",
            "--------------------\n",
            "Epoch 8/30\n",
            "Training Loss: 0.9404 Acc: 0.9720\n",
            "Validation Loss: 1.5696 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 9/30\n",
            "Training Loss: 0.9459 Acc: 0.9708\n",
            "Validation Loss: 1.5612 Acc: 0.7230\n",
            "--------------------\n",
            "Epoch 10/30\n",
            "Training Loss: 0.9384 Acc: 0.9754\n",
            "Validation Loss: 1.5723 Acc: 0.7128\n",
            "--------------------\n",
            "Epoch 11/30\n",
            "Training Loss: 0.9205 Acc: 0.9799\n",
            "Validation Loss: 1.5344 Acc: 0.7297\n",
            "--------------------\n",
            "Epoch 12/30\n",
            "Training Loss: 0.9148 Acc: 0.9826\n",
            "Validation Loss: 1.5348 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 13/30\n",
            "Training Loss: 0.9082 Acc: 0.9818\n",
            "Validation Loss: 1.5422 Acc: 0.7331\n",
            "--------------------\n",
            "Epoch 14/30\n",
            "Training Loss: 0.9015 Acc: 0.9883\n",
            "Validation Loss: 1.5358 Acc: 0.7399\n",
            "--------------------\n",
            "Epoch 15/30\n",
            "Training Loss: 0.9055 Acc: 0.9830\n",
            "Validation Loss: 1.5280 Acc: 0.7432\n",
            "Early stopping triggered\n",
            "Best val Acc: 0.7635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 3: Unfreeze all layers, remove criterion/change? and change for the scheduler the decay to a lower value"
      ],
      "metadata": {
        "id": "JGDfe-TbThOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model().to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True  # Unfreeze all layers\n"
      ],
      "metadata": {
        "id": "294iBjdITLI6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3},       # Higher learning rate for final layer\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4},   # Intermediate for higher layers\n",
        "    {'params': model.layer3.parameters(), 'lr': 1e-5}    # Lower for earlier layers\n",
        "])\n",
        "# Learning rate scheduler to decay LR over time\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Decay LR more frequently for fine-tuning\n",
        "num_epochs = 20\n",
        "patience = 10\n"
      ],
      "metadata": {
        "id": "WQPqAPFvUT6M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path='resnet18-v4.pth'\n",
        "\n",
        "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    patience=patience\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJikN84fUUV8",
        "outputId": "96a8d7fa-a8a8-4172-88da-db92f4af9438"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Training Loss: 1.8976 Acc: 0.6413\n",
            "Validation Loss: 1.5674 Acc: 0.7264\n",
            "--------------------\n",
            "Epoch 2/20\n",
            "Training Loss: 0.9577 Acc: 0.9629\n",
            "Validation Loss: 1.4148 Acc: 0.7669\n",
            "--------------------\n",
            "Epoch 3/20\n",
            "Training Loss: 0.8527 Acc: 0.9837\n",
            "Validation Loss: 1.3970 Acc: 0.7703\n",
            "--------------------\n",
            "Epoch 4/20\n",
            "Training Loss: 0.8060 Acc: 0.9970\n",
            "Validation Loss: 1.3391 Acc: 0.8243\n",
            "--------------------\n",
            "Epoch 5/20\n",
            "Training Loss: 0.7911 Acc: 0.9955\n",
            "Validation Loss: 1.3271 Acc: 0.7905\n",
            "--------------------\n",
            "Epoch 6/20\n",
            "Training Loss: 0.7742 Acc: 0.9992\n",
            "Validation Loss: 1.3049 Acc: 0.8345\n",
            "--------------------\n",
            "Epoch 7/20\n",
            "Training Loss: 0.7672 Acc: 0.9985\n",
            "Validation Loss: 1.2842 Acc: 0.8311\n",
            "--------------------\n",
            "Epoch 8/20\n",
            "Training Loss: 0.7647 Acc: 0.9985\n",
            "Validation Loss: 1.2855 Acc: 0.8345\n",
            "--------------------\n",
            "Epoch 9/20\n",
            "Training Loss: 0.7576 Acc: 1.0000\n",
            "Validation Loss: 1.2771 Acc: 0.8311\n",
            "--------------------\n",
            "Epoch 10/20\n",
            "Training Loss: 0.7573 Acc: 0.9996\n",
            "Validation Loss: 1.2453 Acc: 0.8649\n",
            "--------------------\n",
            "Epoch 11/20\n",
            "Training Loss: 0.7490 Acc: 1.0000\n",
            "Validation Loss: 1.2851 Acc: 0.8378\n",
            "--------------------\n",
            "Epoch 12/20\n",
            "Training Loss: 0.7450 Acc: 1.0000\n",
            "Validation Loss: 1.2619 Acc: 0.8480\n",
            "--------------------\n",
            "Epoch 13/20\n",
            "Training Loss: 0.7432 Acc: 1.0000\n",
            "Validation Loss: 1.2522 Acc: 0.8649\n",
            "--------------------\n",
            "Epoch 14/20\n",
            "Training Loss: 0.7414 Acc: 1.0000\n",
            "Validation Loss: 1.2412 Acc: 0.8615\n",
            "--------------------\n",
            "Epoch 15/20\n",
            "Training Loss: 0.7405 Acc: 1.0000\n",
            "Validation Loss: 1.2429 Acc: 0.8649\n",
            "--------------------\n",
            "Epoch 16/20\n",
            "Training Loss: 0.7402 Acc: 1.0000\n",
            "Validation Loss: 1.2651 Acc: 0.8480\n",
            "--------------------\n",
            "Epoch 17/20\n",
            "Training Loss: 0.7409 Acc: 0.9989\n",
            "Validation Loss: 1.2561 Acc: 0.8615\n",
            "--------------------\n",
            "Epoch 18/20\n",
            "Training Loss: 0.7381 Acc: 1.0000\n",
            "Validation Loss: 1.2456 Acc: 0.8750\n",
            "--------------------\n",
            "Epoch 19/20\n",
            "Training Loss: 0.7349 Acc: 1.0000\n",
            "Validation Loss: 1.2597 Acc: 0.8649\n",
            "--------------------\n",
            "Epoch 20/20\n",
            "Training Loss: 0.7364 Acc: 1.0000\n",
            "Validation Loss: 1.2458 Acc: 0.8682\n",
            "--------------------\n",
            "Best val Acc: 0.8750\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6274954,
          "sourceId": 10161805,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}